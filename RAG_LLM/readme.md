# ðŸ§  RAG-Powered LLM with LangChain âš™ï¸ðŸ”—

![LangChain Logo](https://img.shields.io/badge/LangChain-Enabled-green)
![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Status](https://img.shields.io/badge/Project-Complete-brightgreen)

> ðŸš€ **Retrieval-Augmented Generation (RAG)** combines the power of **LLMs** with **custom document retrieval** for smarter, factual, and context-aware answers.

---

## ðŸ–¼ï¸ Project Demo Screenshots

| Vector Store | Document Retrieval | Final Answer |
|--------------|--------------------|---------------|
| ![Vectorstore](output_1_vectorstore.png) | ![Retrieval](output_2_retrieval.png) | ![Answer](output_3_answer_generation.png) |

---

## ðŸ“Œ Overview

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using:
- âœ… `LangChain` for chaining LLMs & retrieval logic
- âœ… `Chroma` vector store for storing embeddings
- âœ… `OpenAI Embeddings` or any LLM provider for QnA

---

## ðŸ§  RAG Architecture

![RAG Architecture](rag_architecture.png)

> The RAG system retrieves relevant chunks from your knowledge base using semantic search, then feeds them into an LLM to generate accurate and context-specific answers.

---

## ðŸ§° Tech Stack

| Tool | Role |
|------|------|
| **LangChain** | Chaining LLM with retrieval |
| **Chroma** | Vector database for document search |
| **HuggingFace** | LLM provider |
| **Google Colab** | Interactive development |
| **Python** | Core logic implementation |

---

   
