# 🧠 RAG-Powered LLM with LangChain ⚙️🔗

![LangChain Logo](https://img.shields.io/badge/LangChain-Enabled-green)
![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Status](https://img.shields.io/badge/Project-Complete-brightgreen)

> 🚀 **Retrieval-Augmented Generation (RAG)** combines the power of **LLMs** with **custom document retrieval** for smarter, factual, and context-aware answers.

---

## 🖼️ Project Demo Screenshots

| Vector Store | Document Retrieval | Final Answer |
|--------------|--------------------|---------------|
| ![Vectorstore](output_1_vectorstore.png) | ![Retrieval](output_2_retrieval.png) | ![Answer](output_3_answer_generation.png) |

---

## 📌 Overview

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using:
- ✅ `LangChain` for chaining LLMs & retrieval logic
- ✅ `Chroma` vector store for storing embeddings
- ✅ `OpenAI Embeddings` or any LLM provider for QnA

---

## 🧠 RAG Architecture

![RAG Architecture](rag_architecture.png)

> The RAG system retrieves relevant chunks from your knowledge base using semantic search, then feeds them into an LLM to generate accurate and context-specific answers.

---

## 🧰 Tech Stack

| Tool | Role |
|------|------|
| **LangChain** | Chaining LLM with retrieval |
| **Chroma** | Vector database for document search |
| **HuggingFace** | LLM provider |
| **Google Colab** | Interactive development |
| **Python** | Core logic implementation |

---

   
